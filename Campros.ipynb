{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hExK9OhQtF2-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "from torchmetrics import F1Score\n",
        "import seaborn as snn\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h_HE-WvtpWZ",
        "outputId": "3769e223-5370-4f25-8d58-5c1a0a784a5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8450665e90>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Random Seed\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Kv4oa3HPTB"
      },
      "outputs": [],
      "source": [
        "#Training Files\n",
        "df1 = pd.read_json(\"en-train.json\", lines=True)\n",
        "df2 = pd.read_json(\"es-train.json\", lines=True)\n",
        "df3 = pd.read_json(\"pr-train.json\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx59gVDuT9eZ"
      },
      "outputs": [],
      "source": [
        "#Compiling Training Data\n",
        "df = pd.concat([df1, df2 , df3], axis=0)\n",
        "df = df.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJXmM23qhhpE"
      },
      "outputs": [],
      "source": [
        "import html\n",
        "import re\n",
        "\n",
        "#Text Cleaning\n",
        "def clean_text(df):\n",
        "    df['text'] = df['text'].apply(html.unescape)\n",
        "    df['text'] = df['text'].apply(lambda s: re.sub('@\\w+', ' ', s))\n",
        "    df['text'] = df['text'].apply(lambda s: re.sub('#',    ' ', s))\n",
        "    df['text'] = df['text'].apply(lambda s: re.sub('\\n',   ' ', s))\n",
        "    df['text'] = df['text'].apply(lambda s: re.sub('\\w+://\\S+',  ' ', s))\n",
        "    df['text'] = df['text'].apply(lambda s: re.sub('\\s+',  ' ', s))\n",
        "    return df\n",
        "    \n",
        "df = clean_text(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gW1JeQVhhpE",
        "outputId": "74737749-3635-461d-a310-fe524bff2dd0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8151</th>\n",
              "      <td>108151</td>\n",
              "      <td>District faces fallout, say agents PUBLISHED :...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>95433</td>\n",
              "      <td>En plenas bodas de plata de La venganza será t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2154</th>\n",
              "      <td>102154</td>\n",
              "      <td>Sonia to appoint CLP leader TNN | Aug 28, 2001...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>90727</td>\n",
              "      <td>21/10/2002 - 21h17 Termina greve geral contra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6270</th>\n",
              "      <td>106270</td>\n",
              "      <td>China LNG shares dumped as its prospects defla...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>90960</td>\n",
              "      <td>27/05/2002 - 14h39 Veja as últimas pesquisas ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5191</th>\n",
              "      <td>105191</td>\n",
              "      <td>Former V-C walks out of meet over affiliation ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>105390</td>\n",
              "      <td>DEVELOPMENT Sports officials angry over plan t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>100860</td>\n",
              "      <td>Sinha confident of achieving indirect tax targ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7270</th>\n",
              "      <td>107270</td>\n",
              "      <td>Sanjeev Kapoor's Vegetarian Spread for Modi in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11811 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                               text  label\n",
              "8151  108151  District faces fallout, say agents PUBLISHED :...      0\n",
              "433    95433  En plenas bodas de plata de La venganza será t...      0\n",
              "2154  102154  Sonia to appoint CLP leader TNN | Aug 28, 2001...      0\n",
              "727    90727   21/10/2002 - 21h17 Termina greve geral contra...      1\n",
              "6270  106270  China LNG shares dumped as its prospects defla...      0\n",
              "...      ...                                                ...    ...\n",
              "960    90960   27/05/2002 - 14h39 Veja as últimas pesquisas ...      0\n",
              "5191  105191  Former V-C walks out of meet over affiliation ...      0\n",
              "5390  105390  DEVELOPMENT Sports officials angry over plan t...      0\n",
              "860   100860  Sinha confident of achieving indirect tax targ...      0\n",
              "7270  107270  Sanjeev Kapoor's Vegetarian Spread for Modi in...      0\n",
              "\n",
              "[11811 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YG-zWH1u9Vi"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=29)\n",
        "#train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df.label)\n",
        "test_df = pd.read_json(\"Portuguese_test.json\", lines=True)\n",
        "# train_df = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xSTDRCIvGxN"
      },
      "outputs": [],
      "source": [
        "label_column = [\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnZhzORmxBOS"
      },
      "outputs": [],
      "source": [
        "#Samples\n",
        "sample_row = df.iloc[42]\n",
        "sample_comment = sample_row.text\n",
        "sample_labels = sample_row[label_column]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn8aOqUNV_O6"
      },
      "source": [
        "Change model name for different BERT models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WReoOnBJwNUi"
      },
      "outputs": [],
      "source": [
        "#Choosing Model and Tokenizer\n",
        "Bert_Model = \"xlm-roberta-base\" \n",
        "tokenizer = transformers.XLMRobertaTokenizer.from_pretrained(Bert_Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Sp3dM9w6hP"
      },
      "outputs": [],
      "source": [
        "#Sample Encoding\n",
        "encoding = tokenizer.encode_plus(\n",
        "    sample_comment,\n",
        "    add_special_tokens = True,\n",
        "    max_length = 512,\n",
        "    return_token_type_ids = False,\n",
        "    padding = \"max_length\",\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = \"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6dk_MWex_to"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IWnjPx0yuNq"
      },
      "outputs": [],
      "source": [
        "#Dataset Class\n",
        "class DocumentDataset(torch.utils.data.DocumentDataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: transformers.XLMRobertaTokenizer, max_length: int = 256, testData = False):\n",
        "    \n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.test_data = testData\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "\n",
        "        comment_text = data_row.text\n",
        "        labels = data_row[label_column]\n",
        "\n",
        "        if self.test_data:\n",
        "            labels = []\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            add_special_tokens = True,\n",
        "            max_length = self.max_length,\n",
        "            return_token_type_ids = False,\n",
        "            padding = \"max_length\",\n",
        "            truncation = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = \"pt\",\n",
        "        )\n",
        "\n",
        "        if self.test_data:\n",
        "            return dict(\n",
        "              comment_text = comment_text,\n",
        "              input_ids = encoding[\"input_ids\"].flatten(),\n",
        "              attention_mask = encoding[\"attention_mask\"].flatten()\n",
        "            )       \n",
        "        else:\n",
        "            return dict(\n",
        "              comment_text = comment_text,\n",
        "              input_ids = encoding[\"input_ids\"].flatten(),\n",
        "              attention_mask = encoding[\"attention_mask\"].flatten(),\n",
        "              labels = torch.FloatTensor(labels)\n",
        "            )\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqK-Swqf0Eh2",
        "outputId": "3e883c75-ed50-4a94-9f89-f227044666af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#Initialising Dataset Class and Model\n",
        "train_dataset = DocumentDataset(train_df, tokenizer)\n",
        "sample = train_dataset[0]\n",
        "bert_model = transformers.XLMRobertaModel.from_pretrained(Bert_Model, return_dict = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HTdE9EY0xWG"
      },
      "outputs": [],
      "source": [
        "prediction = bert_model(sample[\"input_ids\"].unsqueeze(dim = 0), sample[\"attention_mask\"].unsqueeze(dim = 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JCQwAP81YOn"
      },
      "outputs": [],
      "source": [
        "#Data Module Class for Organizing Different Datasets for Train/Predict\n",
        "class DocumentDataModule(pl.LightningDocumentDataModule):\n",
        "    def __init__(self, train_df, val_df, test_df, tokenizer, batch_size = 32, max_length = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = DocumentDataset(\n",
        "            self.train_df,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        self.val_dataset = DocumentDataset(\n",
        "            self.val_df,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        self.test_dataset = DocumentDataset(\n",
        "            self.test_df,\n",
        "            self.tokenizer,\n",
        "            self.max_length,\n",
        "            testData = True\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset,\n",
        "            self.batch_size,\n",
        "            shuffle = True,\n",
        "            num_workers = 20\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size = 1,\n",
        "            shuffle = False,\n",
        "            num_workers = 20\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size = 1,\n",
        "            shuffle = False,\n",
        "            num_workers = 20\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL7JGDvM3BrP"
      },
      "outputs": [],
      "source": [
        "#Initialization and Hyperparameters\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "data_module = DocumentDataModule(train_df, val_df, test_df, tokenizer, batch_size)\n",
        "data_module.setup()\n",
        "criterion = torch.nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l76HJsu_8Ijn"
      },
      "outputs": [],
      "source": [
        "#Actual Training Class\n",
        "class ModelTrainEval(pl.LightningModule):\n",
        "    def __init__(self, n_classes: int, steps_per_epoch = None, epochs = None, learning_rate = 5e-5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = transformers.AutoModel.from_pretrained(Bert_Model, return_dict = True)\n",
        "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.criterion = torch.nn.BCELoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels = None):\n",
        "        output = self.bert(input_ids, attention_mask = attention_mask)\n",
        "        #print(output.last_hidden_state.size())\n",
        "        output = self.classifier(output.pooler_output)\n",
        "        #print(output.shape)\n",
        "        output = torch.sigmoid(output)\n",
        "        #print(output.shape)\n",
        "        loss = 0\n",
        "        if labels != None:\n",
        "            loss = self.criterion(output, labels)\n",
        "\n",
        "        return loss, output\n",
        "\n",
        "    def train(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar = True, logger = True)\n",
        "\n",
        "        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n",
        "\n",
        "    def validate(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar = True, logger = True)\n",
        "\n",
        "        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n",
        "\n",
        "    def test(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "\n",
        "        loss, output = self(input_ids, attention_mask)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def end_training(self, outputs):\n",
        "        labels = []\n",
        "        predictions = []\n",
        "\n",
        "        for output in outputs:\n",
        "            for out_labels in output[\"labels\"].detach():\n",
        "                labels.append(out_labels.int())\n",
        "\n",
        "            for out_predictions in output[\"predictions\"].detach():\n",
        "                predictions.append(out_predictions)\n",
        "\n",
        "        labels = torch.stack(labels)\n",
        "        predictions = torch.stack(predictions)\n",
        "\n",
        "    def end_validation(self, outputs):\n",
        "        labels = []\n",
        "        predictions = []\n",
        "\n",
        "        for output in outputs:\n",
        "            for out_labels in output[\"labels\"].detach():\n",
        "                labels.append(out_labels.int())\n",
        "\n",
        "            for out_predictions in output[\"predictions\"].detach():\n",
        "                predictions.append(out_predictions)\n",
        "\n",
        "        labels = torch.stack(labels)\n",
        "        predictions = torch.stack(predictions)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = transformers.AdamW(self.parameters(), lr= self.learning_rate)\n",
        "\n",
        "        warmup_steps = self.steps_per_epoch // 3\n",
        "        total_steps = self.steps_per_epoch * self.epochs - warmup_steps\n",
        "\n",
        "        scheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            warmup_steps,\n",
        "            total_steps\n",
        "        )\n",
        "\n",
        "        return [optimizer], [scheduler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTHB-sSPBzyH",
        "outputId": "eadc65e1-d9e4-48cd-c334-da60b0c2c979"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#Initialising Model with required Hyperparameters\n",
        "model = ModelTrainEval(n_classes= 1, steps_per_epoch= len(train_df) // batch_size, epochs= epochs)\n",
        "model.learning_rate = 2.75e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GewimwjkB_X3"
      },
      "outputs": [],
      "source": [
        "_, prediction = model(sample[\"input_ids\"].unsqueeze(dim = 0), sample[\"attention_mask\"].unsqueeze(dim = 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDamSeAPCaJV",
        "outputId": "40e3691f-0b46-4658-b3a1-da6872106e80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch-cb/mrinal20222/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "#Initializing Trainer for Model Training\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.callbacks import  ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n",
        "\n",
        "trainer = pl.Trainer(max_epochs= epochs, gpus = 1,\n",
        "                     callbacks=[\n",
        "                                EarlyStopping(monitor='val_loss', patience=2), \n",
        "                                checkpoint_callback,\n",
        "                                ]\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIipy_xMTw2X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkdQjshnhhpN",
        "outputId": "a843cb4b-bc36-413c-e38c-83ad03fb49d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Save/Load Models\n",
        "#torch.save(model.state_dict(), \"all_train.pt\")\n",
        "model.load_state_dict(torch.load(\"XLMR_all_lang.pt\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_2YUV522imT"
      },
      "outputs": [],
      "source": [
        "model.freeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLPyWHIJ69-m"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lpZep42z274W"
      },
      "outputs": [],
      "source": [
        "#Storing True labels and Prediction Labels for Val Dataset\n",
        "predictions = []\n",
        "targets = []\n",
        "for testObj in data_module.val_dataloader():\n",
        "    _, test_pred = model(testObj[\"input_ids\"], testObj[\"attention_mask\"])\n",
        "    targets.append(testObj[\"labels\"])\n",
        "    predictions.append(test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KdXUYBfs4lyU"
      },
      "outputs": [],
      "source": [
        "prediction_np = []\n",
        "for pred in predictions:\n",
        "    prediction_np.append(pred.flatten().numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SmHr1m445p7W"
      },
      "outputs": [],
      "source": [
        "targets_np = []\n",
        "for t in targets:\n",
        "    targets_np.append(int(t.flatten().numpy()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oBIbC1wQ52rb"
      },
      "outputs": [],
      "source": [
        "#Function to find Threshold for best f1 score and the best f1 score.\n",
        "def max_f1(target, prediction):\n",
        "    f1_max = 0\n",
        "    threshold = 0\n",
        "    for i in range(1, 99, 1):\n",
        "        score = sklearn.metrics.f1_score(np.array(target), np.array(prediction) > (i / 100), average= \"macro\")\n",
        "\n",
        "        if score > f1_max:\n",
        "            f1_max = score\n",
        "            threshold = i\n",
        "            \n",
        "    return threshold, f1_max \n",
        "\n",
        "print(f\"Threshold = {threshold}, F1 Score = {f1_max}\")\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FAa4JDvzOhZ"
      },
      "outputs": [],
      "source": [
        "#Storing Predictions of Test Dataset\n",
        "predictions = []\n",
        "for testObj in data_module.test_dataloader():\n",
        "    _, test_pred = model(testObj[\"input_ids\"], testObj[\"attention_mask\"])\n",
        "    predictions.append(test_pred)\n",
        "\n",
        "prediction_np = []\n",
        "for pred in predictions:\n",
        "    prediction_np.append(pred.cpu().flatten().numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIS3EhDvhhpO"
      },
      "outputs": [],
      "source": [
        "prediction_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVnOYrqphhpP"
      },
      "outputs": [],
      "source": [
        "len(prediction_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gziD9SX8hhpP"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS_ErPdQhhpP"
      },
      "outputs": [],
      "source": [
        "ans = [1 if i>=0.6 else 0 for i in prediction_np]\n",
        "ans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvCP99XEhhpP"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "json_list = []\n",
        "for id in test_df[\"id\"]:\n",
        "    sub = {}\n",
        "    sub[\"prediction\"]=ans[count]\n",
        "    sub[\"id\"] = id\n",
        "    count+=1\n",
        "    print(str(sub))\n",
        "    json_list.append(sub)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGLhoDeyhhpP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"Portuguese_XLMR_Softmax_Submission.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            for doc in json_list:\n",
        "                f.write(json.dumps(doc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPEzD3YPhhpP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpZuyjE2hhpP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}